{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import benepar\n",
    "from typing import List\n",
    "from spacy.matcher import Matcher\n",
    "from concept_processing.nlp.spacy_wrapper import SpacyWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /Users/Cherry0904/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benepar.download('benepar_en3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_atomisation(atomic_sents: List[str]) -> List[str]:\n",
    "    # nlp = SpacyWrapper()\n",
    "    nlp = en_core_web_sm.load()\n",
    "    excmatcher = add_exc_matcher(nlp)\n",
    "\n",
    "    num_of_words = [len(sent.split()) for sent in atomic_sents]\n",
    "    index_of_short_concepts = [i for i, j in enumerate(num_of_words) if j == 2 or j==3]\n",
    "    index_of_all_concepts = [i for i, j in enumerate(num_of_words)]\n",
    "    index_of_long_concepts = [x for x in index_of_all_concepts if x not in index_of_short_concepts]\n",
    "    # index_of_long_concepts = [i for i, j in enumerate(num_of_words) if j != 2 and j !=3]\n",
    "    print(index_of_long_concepts)\n",
    "    # index_of_3_token_concepts = [i for i, j in enumerate(num_of_words) if j == 3]\n",
    "        \n",
    "    index_to_keep = []\n",
    "    for i in index_of_short_concepts:\n",
    "        doc = nlp(atomic_sents[i])\n",
    "        match = excmatcher(doc)\n",
    "\n",
    "        if match == []: # If there is no match\n",
    "            index_to_keep.append(i)\n",
    "\n",
    "    atomic_sents = [atomic_sents[i] for i in index_to_keep + index_of_long_concepts]\n",
    "\n",
    "    return atomic_sents\n",
    "\n",
    "\n",
    "\n",
    "def add_exc_matcher(nlp):\n",
    "    # create exclusion matcher for our concepts\n",
    "    excpattern1 = [{\"POS\": {\"IN\": [\"NOUN\", \"PRON\"]}}, {\"POS\": \"VERB\"}] # such as \"it looks\"\n",
    "    # excpattern2 = [{\"POS\": \"DET\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"VERB\"}] # such as \"the woman looks\"\n",
    "        \n",
    "    # Dirty way of using SpacyWrapper, kept because this code is not maintained\n",
    "    # excmatcher = Matcher(nlp._nlp.vocab)\n",
    "    excmatcher = Matcher(nlp.vocab)\n",
    "\n",
    "    excmatcher.add(\"short_concept_2_tokens\", [excpattern1])\n",
    "    # excmatcher.add(\"short_concept_3_tokens\", [excpattern2])\n",
    "    # nlp._nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "\n",
    "    return excmatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cherry0904/Desktop/roko-for-charlize/venv/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n",
      "['Happy birthday.', 'Love you.', 'Brilliant.']\n"
     ]
    }
   ],
   "source": [
    "atomic_sents = [\"It looks.\", \"She looks.\", \"Me laugh.\", \"The people look.\", \"The colors make.\", \"Happy birthday.\", \"Brilliant.\", \"Love you.\"]\n",
    "post_processed_atomic_sents = post_process_atomisation(atomic_sents)\n",
    "print(post_processed_atomic_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "excmatcher = add_exc_matcher(nlp)\n",
    "\n",
    "atomic_sents = [\"Happy.\"]\n",
    "doc = nlp(atomic_sents[0])\n",
    "match = excmatcher(doc)\n",
    "print(match)\n",
    "# for match_id, start, end in match:\n",
    "#     span = doc[start:end]\n",
    "#     print(start, end, span)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in match:\n",
    "    span = doc[start:end]\n",
    "    print(start, end, span)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ac8d745845c1c2f5d4be80af942cfba6164db4980267ad6e5838852de960463"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
